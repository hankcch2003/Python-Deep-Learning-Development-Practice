{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZiybaJmoKna"
   },
   "source": [
    "# 以 Monte Carlo 求解 FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1756215964807,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "8Z2fsdYLoKni"
   },
   "outputs": [],
   "source": [
    "# 匯入numpy套件，用於數值計算與陣列處理\n",
    "import numpy as np\n",
    "\n",
    "# 匯入random模組，用於隨機數生成\n",
    "import random\n",
    "\n",
    "# 匯入sys模組，用於與系統互動\n",
    "import sys\n",
    "\n",
    "# 匯入gymnasium套件，用於建立和操作強化學習環境\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1756215971914,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "XbXqTzjzoKnk"
   },
   "outputs": [],
   "source": [
    "# 建立FrozenLake-v1環境，is_slippery = False表示湖面不滑(動作是確定性的)\n",
    "# 預設地圖大小為4x4，若要切換為較大的地圖可加入參數map_name = \"8x8\"\n",
    "env = gym.make('FrozenLake-v1', is_slippery = False)  # , map_name = \"8x8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1756216293623,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "aK9UaeZioKnl"
   },
   "outputs": [],
   "source": [
    "# 建立隨機策略函式(每個狀態下的每個動作機率平均)\n",
    "def create_random_policy(env):\n",
    "    # 建立一個空字典，用來儲存每個狀態對應的動作機率分布\n",
    "    policy = {}\n",
    "\n",
    "    # 逐一遍歷所有狀態(從0到狀態數-1)\n",
    "    for state in range(env.observation_space.n):\n",
    "        # 建立一個空字典，儲存該狀態下每個動作的機率\n",
    "        action_probabilities = {}\n",
    "\n",
    "        # 逐一遍歷所有可能動作(從0到動作數-1)\n",
    "        for action in range(env.action_space.n):\n",
    "            # 對於每個動作，分配相同的機率(均勻分布，機率 = 1 / 動作總數)\n",
    "            action_probabilities[action] = 1 / env.action_space.n\n",
    "\n",
    "        # 將該狀態下的動作機率分布加入整體策略字典中\n",
    "        policy[state] = action_probabilities\n",
    "\n",
    "    # 傳回整個隨機策略(每個狀態對應一個動作機率分布)\n",
    "    return policy\n",
    "\n",
    "# 建立狀態-行動Q值表(初始為0.0)\n",
    "def create_state_action_dictionary(env, policy):\n",
    "    # 建立一個空字典，用來儲存每個狀態-動作對應的Q值\n",
    "    Q = {}\n",
    "\n",
    "    # 遍歷策略中的每個狀態(也就是所有可能的狀態)\n",
    "    for state in policy.keys():\n",
    "        # 對於該狀態，建立一個動作字典，初始每個動作的Q值都為0.0\n",
    "        Q[state] = {action: 0.0 for action in range(env.action_space.n)}\n",
    "\n",
    "    # 傳回Q值表，結構為Q[state][action] = 值\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1756216358529,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "4LdnhkgtoKnm"
   },
   "outputs": [],
   "source": [
    "# 執行一回合遊戲，依據給定的策略(policy)與環境(env)模擬\n",
    "def run_game(env, policy):\n",
    "    s, info = env.reset()  # 重置環境，取得起始狀態s(和環境資訊info)\n",
    "    episode = []           # 儲存整個episode的狀態、動作、獎勵序列\n",
    "    done = False           # 回合是否結束(初始為False)\n",
    "\n",
    "    # 只要回合尚未結束，就持續進行\n",
    "    while not done:\n",
    "        # 儲存單一時間步的[狀態, 動作, 獎勵]\n",
    "        timestep = []\n",
    "\n",
    "        # 將當前狀態加入時間步資料中\n",
    "        timestep.append(s)\n",
    "\n",
    "        # 根據策略進行動作選擇：使用機率分布抽樣一個動作\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "\n",
    "        # 先從0到該狀態所有動作機率總和中隨機抽一個數字\n",
    "        top_range = 0\n",
    "\n",
    "        # 依照機率累加的方式進行抽樣(等同於roulette wheel selection)\n",
    "        for prob in policy[s].items():  # prob是(tuple)，第一個元素是動作action，第二個元素是該動作的機率\n",
    "            top_range += prob[1]        # 將該動作的機率累加到top_range，形成機率區間邊界\n",
    "            if n < top_range:           # 如果隨機數n落在目前累積的機率區間內\n",
    "                action = prob[0]        # 就選擇該動作\n",
    "                break                   # 找到動作後跳出迴圈\n",
    "\n",
    "        # 根據選定的動作與當前狀態與環境互動，取得新狀態與獎勵\n",
    "        s, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # 判斷是否為終止狀態(遊戲結束)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # 將這個時間步的[動作, 獎勵]加入timestep\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        # 將該時間步加入整個episode紀錄\n",
    "        episode.append(timestep)\n",
    "\n",
    "    # 傳回完整的episode\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1756216540015,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "J8plgI6MoKnn"
   },
   "outputs": [],
   "source": [
    "# 定義蒙地卡羅強化學習函式\n",
    "# env：環境物件\n",
    "# episodes：執行的回合數(預設1000)\n",
    "# policy：依據該策略決定每個狀態下執行的動作機率分布(預設None，會建立隨機策略)\n",
    "# epsilon：ε-greedy策略中探索的機率(預設0.01)\n",
    "\n",
    "def monte_carlo(env, episodes = 1000, policy = None, epsilon = 0.01):\n",
    "    # 如果沒有提供策略，則建立一個隨機策略(每個動作均等機率)\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)\n",
    "\n",
    "    # 初始化狀態-行動值函數Q，所有Q值初始為0\n",
    "    Q = create_state_action_dictionary(env, policy)\n",
    "\n",
    "    # 用於儲存狀態-行動對應的回報列表\n",
    "    returns = {}\n",
    "\n",
    "    # 進行指定數量的回合(episodes)\n",
    "    for i_episode in range(episodes):\n",
    "        # 每1000回合輸出進度提示\n",
    "        if (i_episode + 1) % 1000 == 0:\n",
    "            print(f\"\\r {(i_episode + 1)}/{episodes}回合.\", end=\"\")\n",
    "            sys.stdout.flush()  # 清除緩衝區，即時輸出\n",
    "\n",
    "        # 初始化累積報酬為0\n",
    "        G = 0\n",
    "\n",
    "        # 執行一回合遊戲，傳回該回合的(state, action, reward)序列\n",
    "        episode = run_game(env = env, policy = policy)\n",
    "\n",
    "        # 從回合最後一步開始往前計算累積報酬G\n",
    "        for i in reversed(range(len(episode))):\n",
    "            s_t, a_t, r_t = episode[i]  # 取出該時間步的狀態、動作與獎勵\n",
    "            state_action = (s_t, a_t)   # 狀態-動作配對\n",
    "            G += r_t                    # 累積報酬(從末端往前加)\n",
    "\n",
    "            # 若此狀態-動作對是該回合第一次出現(首次訪問MC)\n",
    "            if state_action not in [(x[0], x[1]) for x in episode[0: i]]:\n",
    "                # 如果returns字典已有該狀態-動作的回報列表，則加入G\n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    # 否則建立新的列表並加入G\n",
    "                    returns[state_action] = [G]\n",
    "\n",
    "                # 更新該狀態-動作對的Q值為平均回報\n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action])\n",
    "\n",
    "                # 取得該狀態下所有動作的Q值清單\n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items()))\n",
    "\n",
    "                # 找出Q值最高的動作索引(有多個則隨機選一個)\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "\n",
    "                # 設定ε-greedy策略，給最大Q值動作較大機率\n",
    "                A_star = max_Q\n",
    "                for a in policy[s_t].items():\n",
    "                    if a[0] == A_star:\n",
    "                        # 最佳動作的機率設為1 - ε + (ε平均分配部分)\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        # 其他動作機率均分剩餘ε部分\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    # 傳回更新後的策略\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183941,
     "status": "ok",
     "timestamp": 1756216797221,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "eZyVtFlwoKno",
    "outputId": "0aa19fc1-8483-4aa1-e98c-8369d76c02be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60000/60000回合."
     ]
    }
   ],
   "source": [
    "# 使用蒙地卡羅方法訓練策略，執行60000回合\n",
    "policy = monte_carlo(env, episodes = 60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1756216798631,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "-Lm30uDkoKnp",
    "outputId": "66d7916c-640a-4952-929b-42a5ffac2837"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定義函式用來評估給定策略(policy)在指定環境(env)中成功率\n",
    "def test_policy(policy, env):\n",
    "    wins = 0        # 記錄成功次數(抵達終點)\n",
    "    runs = 1000     # 測試次數\n",
    "\n",
    "    # 執行指定次數的回合數\n",
    "    for _ in range(runs):\n",
    "        # 執行一回合遊戲，run_game傳回的是[(state, action, reward), ...]\n",
    "        # 取最後一筆資料的reward值\n",
    "        final_reward = run_game(env, policy)[-1][-1]\n",
    "\n",
    "        # 如果最後一步的reward是1，代表該回合成功達成目標\n",
    "        if final_reward == 1:\n",
    "            # 成功次數累加\n",
    "            wins += 1\n",
    "\n",
    "    # 傳回成功率(成功次數/總測試次數)\n",
    "    return wins / runs\n",
    "\n",
    "# 呼叫測試函式，傳入訓練好的策略與環境\n",
    "test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1756216806448,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "DT7kFyj5oKnq",
    "outputId": "e19ff3d3-d9df-4461-b8bc-509babc59fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of wins over 1000 episodes = 963\n",
      "average reward over 1000 episodes = 0.963\n"
     ]
    }
   ],
   "source": [
    "# 定義函式play_episodes，讓agent在環境中依照策略執行多回合遊戲並統計結果\n",
    "# environment：環境物件\n",
    "# n_episodes：執行的回合數(預設1000)\n",
    "# policy：依據該策略決定每個狀態下執行的動作機率分布(預設None，會建立隨機策略)\n",
    "\n",
    "def play_episodes(environment, n_episodes = 1000, policy = None):\n",
    "    wins = 0          # 記錄贏得回合數\n",
    "    total_reward = 0  # 總獲得獎勵累積\n",
    "\n",
    "    # 執行指定回合數的測試\n",
    "    for episode in range(n_episodes):\n",
    "        done = False                       # 回合結束旗標\n",
    "        state, info = environment.reset()  # 重置環境並取得初始狀態\n",
    "\n",
    "        # 持續進行直到回合結束\n",
    "        while not done:\n",
    "            # 根據當前狀態從策略中抽樣選擇動作\n",
    "            n = random.uniform(0, sum(policy[state].values()))  # 生成0到總機率間的隨機數\n",
    "            top_range = 0                                       # 初始化累積機率區間邊界，用於後續累加機率進行抽樣\n",
    "\n",
    "            # 使用機率累積方式選擇動作(roulette wheel selection)\n",
    "            for prob in policy[state].items():  # 逐一遍歷該狀態下的所有動作與其機率，prob是(動作, 機率)的tuple\n",
    "                top_range += prob[1]            # 將目前動作的機率累加到top_range，建立累積機率區間邊界\n",
    "                if n < top_range:               # 如果隨機數n落在累積機率區間內，表示選擇此動作\n",
    "                    action = prob[0]            # 將動作從prob中取出，設定為要執行的動作\n",
    "                    break                       # 找到動作後跳出迴圈，不再繼續累加\n",
    "\n",
    "            # 執行選定動作，並取得下一狀態及獎勵\n",
    "            next_state, reward, terminated, truncated, info = environment.step(action)\n",
    "\n",
    "            # 判斷回合是否結束\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # 累積本回合總獎勵\n",
    "            total_reward += reward\n",
    "\n",
    "            # 更新狀態為下一狀態\n",
    "            state = next_state\n",
    "\n",
    "            # 若回合結束且獲得成功獎勵(1.0)，贏次數加一\n",
    "            if done and reward == 1.0:\n",
    "                wins += 1\n",
    "\n",
    "    # 計算每回合平均獎勵\n",
    "    average_reward = total_reward / n_episodes\n",
    "\n",
    "    # 傳回贏的次數、總獎勵、平均獎勵\n",
    "    return wins, total_reward, average_reward\n",
    "\n",
    "# 測試程式碼：執行1000回合並輸出結果\n",
    "n_episodes = 1000\n",
    "wins, total_reward, average_reward = play_episodes(env, n_episodes, policy)\n",
    "print(f'number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'average reward over {n_episodes} episodes = {average_reward}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
