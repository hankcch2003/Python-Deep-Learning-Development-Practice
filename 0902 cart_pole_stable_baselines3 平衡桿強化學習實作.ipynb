{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMKWTd0zy8UM"
   },
   "source": [
    "# CartPole 強化學習實作(使用Stable Baselines3 套件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-1xr5ZGxy8Ub"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# 匯入numpy套件，用於數值計算與陣列處理\n",
    "import numpy as np\n",
    "\n",
    "# 解決numpy版本更新後移除np.bool和np.bool8的問題\n",
    "if not hasattr(np, 'bool'):   # 如果np模組中沒有bool屬性，則將其指向內建的bool類型\n",
    "    np.bool = bool\n",
    "if not hasattr(np, 'bool8'):  # 如果np模組中沒有bool8屬性，則將其指向np.bool\n",
    "    np.bool8 = np.bool\n",
    "\n",
    "# 匯入warnings模組，用於忽略不必要的警告訊息\n",
    "import warnings\n",
    "\n",
    "# 忽略所有的警告訊息，讓輸出結果更乾淨\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 匯入gymnasium套件，用於建立和操作強化學習環境\n",
    "import gymnasium as gym\n",
    "\n",
    "# 匯入A2C演算法，用於執行Actor-Critic強化學習訓練\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# 匯入make_vec_env函式，用於建立多重環境以進行並行訓練\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# 匯入SubprocVecEnv類別，用於在多個子進程中平行運行環境\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4ir7y3bZ8-H-"
   },
   "outputs": [],
   "source": [
    "# 建立一個包含4個並行實例的CartPole-v1環境\n",
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs = 4)\n",
    "\n",
    "# 建立A2C模型，使用多層感知器策略(MlpPolicy)作為策略網路\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose = 0)\n",
    "\n",
    "# 開始訓練模型，總訓練步數為10000\n",
    "model.learn(total_timesteps = 10000)\n",
    "\n",
    "# 儲存訓練完成的模型\n",
    "model.save(\"a2c_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 刪除模型以釋放記憶體資源\n",
    "del model\n",
    "\n",
    "# 測試用布林值列表，計算其中True的總數\n",
    "test = [False, False, False, False]\n",
    "\n",
    "# 輸出True的個數(True = 1，False = 0)\n",
    "sum(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 304641,
     "status": "error",
     "timestamp": 1744793788365,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "-eI4kqAk9CRh",
    "outputId": "33085dda-e4e3-4ede-9b5b-eea76418470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: 8000.0\n",
      "Episode 2 reward: 8000.0\n",
      "Episode 3 reward: 8000.0\n",
      "Episode 4 reward: 8000.0\n",
      "Episode 5 reward: 8000.0\n",
      "Episode 6 reward: 8000.0\n",
      "Episode 7 reward: 8000.0\n",
      "Episode 8 reward: 8000.0\n",
      "Episode 9 reward: 8000.0\n",
      "Episode 10 reward: 8000.0\n",
      "Episode 11 reward: 8000.0\n",
      "Episode 12 reward: 8000.0\n",
      "Episode 13 reward: 8000.0\n",
      "Episode 14 reward: 8000.0\n",
      "Episode 15 reward: 8000.0\n",
      "Episode 16 reward: 8000.0\n",
      "Episode 17 reward: 8000.0\n",
      "Episode 18 reward: 8000.0\n",
      "Episode 19 reward: 8000.0\n",
      "Episode 20 reward: 8000.0\n",
      "Episode 21 reward: 8000.0\n",
      "Episode 22 reward: 8000.0\n",
      "Episode 23 reward: 8000.0\n",
      "Episode 24 reward: 8000.0\n",
      "Episode 25 reward: 8000.0\n",
      "Episode 26 reward: 8000.0\n",
      "Episode 27 reward: 8000.0\n",
      "Episode 28 reward: 8000.0\n",
      "Episode 29 reward: 8000.0\n",
      "Episode 30 reward: 8000.0\n",
      "Episode 31 reward: 8000.0\n",
      "Episode 32 reward: 8000.0\n",
      "Episode 33 reward: 8000.0\n",
      "Episode 34 reward: 8000.0\n",
      "Episode 35 reward: 8000.0\n",
      "Episode 36 reward: 8000.0\n",
      "Episode 37 reward: 8000.0\n",
      "Episode 38 reward: 8000.0\n",
      "Episode 39 reward: 8000.0\n",
      "Episode 40 reward: 8000.0\n",
      "Episode 41 reward: 8000.0\n",
      "Episode 42 reward: 8000.0\n",
      "Episode 43 reward: 8000.0\n",
      "Episode 44 reward: 8000.0\n",
      "Episode 45 reward: 8000.0\n",
      "Episode 46 reward: 8000.0\n",
      "Episode 47 reward: 8000.0\n",
      "Episode 48 reward: 8000.0\n",
      "Episode 49 reward: 8000.0\n",
      "Episode 50 reward: 8000.0\n",
      "Episode 51 reward: 8000.0\n",
      "Episode 52 reward: 8000.0\n",
      "Episode 53 reward: 8000.0\n",
      "Episode 54 reward: 8000.0\n",
      "Episode 55 reward: 8000.0\n",
      "Episode 56 reward: 8000.0\n",
      "Episode 57 reward: 8000.0\n",
      "Episode 58 reward: 8000.0\n",
      "Episode 59 reward: 8000.0\n",
      "Episode 60 reward: 8000.0\n",
      "Episode 61 reward: 8000.0\n",
      "Episode 62 reward: 8000.0\n",
      "Episode 63 reward: 8000.0\n",
      "Episode 64 reward: 8000.0\n",
      "Episode 65 reward: 8000.0\n",
      "Episode 66 reward: 8000.0\n",
      "Episode 67 reward: 8000.0\n",
      "Episode 68 reward: 8000.0\n",
      "Episode 69 reward: 8000.0\n",
      "Episode 70 reward: 8000.0\n",
      "Episode 71 reward: 8000.0\n",
      "Episode 72 reward: 8000.0\n",
      "Episode 73 reward: 8000.0\n",
      "Episode 74 reward: 8000.0\n",
      "Episode 75 reward: 8000.0\n",
      "Episode 76 reward: 8000.0\n",
      "Episode 77 reward: 8000.0\n",
      "Episode 78 reward: 8000.0\n",
      "Episode 79 reward: 8000.0\n",
      "Episode 80 reward: 8000.0\n",
      "Episode 81 reward: 8000.0\n",
      "Episode 82 reward: 8000.0\n",
      "Episode 83 reward: 8000.0\n",
      "Episode 84 reward: 8000.0\n",
      "Episode 85 reward: 8000.0\n",
      "Episode 86 reward: 8000.0\n",
      "Episode 87 reward: 8000.0\n",
      "Episode 88 reward: 8000.0\n",
      "Episode 89 reward: 8000.0\n",
      "Episode 90 reward: 8000.0\n",
      "Episode 91 reward: 8000.0\n",
      "Episode 92 reward: 8000.0\n",
      "Episode 93 reward: 8000.0\n",
      "Episode 94 reward: 8000.0\n",
      "Episode 95 reward: 8000.0\n",
      "Episode 96 reward: 8000.0\n",
      "Episode 97 reward: 8000.0\n",
      "Episode 98 reward: 8000.0\n",
      "Episode 99 reward: 8000.0\n",
      "Episode 100 reward: 8000.0\n"
     ]
    }
   ],
   "source": [
    "# 載入先前儲存的A2C模型\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "# 用於紀錄每回合的總報酬(每個回合是4個環境的獎勵總和)\n",
    "all_rewards = []\n",
    "\n",
    "# 初始化當前回合的總報酬(用numpy陣列，對應4個環境)\n",
    "total_rewards = np.zeros(4)\n",
    "\n",
    "# 重置並行環境並取得初始狀態\n",
    "obs = vec_env.reset()\n",
    "\n",
    "# 設定測試的回合數(預設為100)，每個回合會執行完整環境直到完成(直到done為True)\n",
    "test_episodes = 100\n",
    "\n",
    "# 設定每回合最大步數限制(預設為2000)，避免無限迴圈問題\n",
    "max_steps_per_episode = 2000\n",
    "\n",
    "# 記錄目前回合已執行的步數\n",
    "step_count = 0\n",
    "\n",
    "# 持續執行測試直到所有回合結束，確保測試完整\n",
    "while test_episodes > 0:\n",
    "    # 使用模型根據觀測值選擇動作(deterministic = True表示採用確定性策略)\n",
    "    action, _state = model.predict(obs, deterministic = True)\n",
    "    \n",
    "    # 根據選定的動作與環境互動，取得新的觀測值、獎勵、完成狀態與額外資訊\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    \n",
    "    # 累積本回合的獎勵\n",
    "    total_rewards += reward\n",
    "    \n",
    "    # 計算目前回合的步數\n",
    "    step_count += 1\n",
    "\n",
    "    # 若所有環境回合結束(4個環境皆done)或達到最大步數限制，則輸出本回合的累積獎勵\n",
    "    if sum(done) == 4 or step_count >= max_steps_per_episode:\n",
    "        print(\"Episode\", 101 - test_episodes, \"reward:\", total_rewards.sum())\n",
    "        \n",
    "        # 將本回合累積的總報酬加入紀錄清單中\n",
    "        all_rewards.append(total_rewards.sum())\n",
    "        \n",
    "        # 初始化當前回合的總報酬(用numpy陣列，對應4個環境)\n",
    "        total_rewards = np.zeros(4)\n",
    "\n",
    "        # 重置步數計數器，開始新回合的步數計算\n",
    "        step_count = 0\n",
    "\n",
    "        # 將剩餘的測試回合數減1，避免無限迴圈\n",
    "        test_episodes -= 1\n",
    "\n",
    "# 關閉環境並釋放資源\n",
    "vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "l24I1n1Yy8Up"
   },
   "outputs": [],
   "source": [
    "# 程式進入點，確保子進程能正確啟動(避免Windows等系統問題)\n",
    "if __name__ == \"__main__\":\n",
    "    # 建立4個子進程平行運行的CartPole-v1環境\n",
    "    env = make_vec_env(\"CartPole-v1\", n_envs = 4, vec_env_cls = SubprocVecEnv)\n",
    "\n",
    "    # 建立A2C模型，使用多層感知器策略，指定運算設備為CPU\n",
    "    model = A2C(\"MlpPolicy\", env, device=\"cpu\")\n",
    "\n",
    "    # 執行訓練(總共進行10000個時間步)\n",
    "    model.learn(total_timesteps = 10000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
