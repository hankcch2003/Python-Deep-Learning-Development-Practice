{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2gKXfPW0cPP"
   },
   "source": [
    "# 木棒台車遊戲測試(CartPole testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1755782964732,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "S6yIzR6s0cPX"
   },
   "outputs": [],
   "source": [
    "# 匯入Gymnasium套件，用來建立與操作強化學習環境\n",
    "import gymnasium as gym\n",
    "\n",
    "# 載入Gymnasium套件中的木棒台車(CartPole)環境\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# 參數初始化\n",
    "no = 50            # 比賽回合數(總共模擬50次遊戲)\n",
    "all_rewards = []   # 每回合的累積報酬會儲存在這個list中\n",
    "all_steps = []     # 每回合的步數也會儲存在這個list中\n",
    "total_rewards = 0  # 單一回合累積報酬\n",
    "total_steps = 0    # 單一回合累積步數\n",
    "\n",
    "# 環境初始化\n",
    "observation, info = env.reset()  # 重置環境並取得初始觀測值\n",
    "\n",
    "# 主迴圈：進行50回合模擬\n",
    "while no > 0:\n",
    "    # 隨機選擇一個動作(在CartPole遊戲中是：向左或向右推)\n",
    "    action = env.action_space.sample()\n",
    "    total_steps += 1\n",
    "\n",
    "    # 將動作送進環境，進行下一步模擬\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # done表示這一回合是否結束(超時或失敗)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # 累計該回合的總報酬\n",
    "    total_rewards += reward\n",
    "\n",
    "    # 若該回合已結束，重置環境，並記錄該回合的報酬與步數\n",
    "    if done:\n",
    "        observation, info = env.reset()    # 重置環境到初始狀態，並傳回額外的資訊\n",
    "        all_rewards.append(total_rewards)  # 儲存該回合的總報酬\n",
    "        all_steps.append(total_steps)      # 儲存該回合的總步數\n",
    "        total_rewards = 0                  # 清空本回合報酬\n",
    "        total_steps = 0                    # 清空本回合步數\n",
    "        no -= 1                            # 剩餘回合數減一\n",
    "\n",
    "# 結束模擬，關閉環境\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1755782964735,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "hGQtp3Nd0cPZ",
    "outputId": "b44c4847-2934-45ad-cdc7-4eba1ceac135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合\t報酬\t結果\n",
      "0\t20.0\tLoss\n",
      "1\t35.0\tLoss\n",
      "2\t16.0\tLoss\n",
      "3\t11.0\tLoss\n",
      "4\t21.0\tLoss\n",
      "5\t24.0\tLoss\n",
      "6\t20.0\tLoss\n",
      "7\t14.0\tLoss\n",
      "8\t32.0\tLoss\n",
      "9\t48.0\tLoss\n",
      "10\t27.0\tLoss\n",
      "11\t13.0\tLoss\n",
      "12\t12.0\tLoss\n",
      "13\t14.0\tLoss\n",
      "14\t24.0\tLoss\n",
      "15\t16.0\tLoss\n",
      "16\t11.0\tLoss\n",
      "17\t15.0\tLoss\n",
      "18\t25.0\tLoss\n",
      "19\t19.0\tLoss\n",
      "20\t15.0\tLoss\n",
      "21\t20.0\tLoss\n",
      "22\t44.0\tLoss\n",
      "23\t11.0\tLoss\n",
      "24\t22.0\tLoss\n",
      "25\t25.0\tLoss\n",
      "26\t16.0\tLoss\n",
      "27\t14.0\tLoss\n",
      "28\t18.0\tLoss\n",
      "29\t17.0\tLoss\n",
      "30\t22.0\tLoss\n",
      "31\t10.0\tLoss\n",
      "32\t10.0\tLoss\n",
      "33\t12.0\tLoss\n",
      "34\t23.0\tLoss\n",
      "35\t18.0\tLoss\n",
      "36\t11.0\tLoss\n",
      "37\t26.0\tLoss\n",
      "38\t23.0\tLoss\n",
      "39\t55.0\tLoss\n",
      "40\t11.0\tLoss\n",
      "41\t23.0\tLoss\n",
      "42\t15.0\tLoss\n",
      "43\t24.0\tLoss\n",
      "44\t30.0\tLoss\n",
      "45\t29.0\tLoss\n",
      "46\t14.0\tLoss\n",
      "47\t23.0\tLoss\n",
      "48\t16.0\tLoss\n",
      "49\t19.0\tLoss\n"
     ]
    }
   ],
   "source": [
    "# 顯示每回合的執行結果，包括回合數、累積報酬和結果判定\n",
    "print('回合\\t報酬\\t結果')\n",
    "\n",
    "# 判斷結果：如果步數達到或超過200，視為勝利，否則為失敗\n",
    "for i, (rewards, steps) in enumerate(zip(all_rewards, all_steps)):\n",
    "    result = 'Win' if steps >= 200 else 'Loss'\n",
    "    \n",
    "    # 輸出回合編號、累積報酬和結果\n",
    "    print(f'{i}\\t{rewards}\\t{result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1755782964736,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "VBj-zBL40cPc"
   },
   "outputs": [],
   "source": [
    "# 匯入math函式，提供數學相關函數與常數\n",
    "import math\n",
    "\n",
    "# 定義台車行進方向，0代表往左，1代表往右\n",
    "left, right = 0, 1\n",
    "\n",
    "# 設定最大角度閾值(度數)，表示杆子相對垂直方向偏離超過多少度時，台車會往偏離的方向移動\n",
    "# 例如，杆子向右傾斜超過8度，台車就往右推動；杆子向左傾斜超過8度，台車就往左推動\n",
    "max_angle = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1755782964739,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "yyiyXR5v0cPc"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # 初始化代理人\n",
    "    def __init__(self):\n",
    "        self.direction = left        # 當前移動方向(0:左、1:右)\n",
    "        self.last_direction = right  # 上一次的移動方向，初始為右(防止一開始就重複)\n",
    "\n",
    "    # 根據觀測值決定下一步動作\n",
    "    def act(self, observation):\n",
    "        # 將觀測值拆解成變數(依序為：台車位置、台車速度、桿子角度、桿子角速度)\n",
    "        cart_position, cart_velocity, pole_angle, pole_velocity = observation\n",
    "\n",
    "        '''\n",
    "        行動策略(自訂邏輯)：\n",
    "        1. 如果桿子角度在 ±8 度以內(偏離不大)：每次行動交替左右移動，避免持續單一方向，讓桿子盡量維持平衡。\n",
    "        2. 如果桿子角度超過 8 度(偏右)：將方向設為右，推回桿子。\n",
    "        3. 如果桿子角度小於 -8 度(偏左)：將方向設為左，推回桿子。\n",
    "        '''\n",
    "\n",
    "        # 如果桿子角度在-8度到+8度之間(尚可接受的偏移範圍)\n",
    "        if pole_angle < math.radians(max_angle) and pole_angle > math.radians(-max_angle):\n",
    "            # 在允許角度範圍內，左右交替行動(避免持續往同一方向推)\n",
    "            self.direction = (self.last_direction + 1) % 2\n",
    "\n",
    "        # 如果桿子角度超過+8度(往右傾斜太多)\n",
    "        elif pole_angle >= math.radians(max_angle):\n",
    "            # 將台車往右推，以幫助桿子回正\n",
    "            self.direction = right\n",
    "\n",
    "        # 剩下情況就是桿子往左傾斜太多(小於-8度)\n",
    "        else:\n",
    "            # 將台車往左推，以幫助桿子回正\n",
    "            self.direction = left\n",
    "\n",
    "        # 儲存這次的行動方向，供下一次判斷是否交替使用\n",
    "        self.last_direction = self.direction\n",
    "\n",
    "        # 回傳這次要執行的動作(0 = 往左，1 = 往右)，供環境執行\n",
    "        return self.direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1755782964889,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "xjpBs_4k0cPd"
   },
   "outputs": [],
   "source": [
    "# 重置環境，取得初始觀測值與環境資訊\n",
    "observation, info = env.reset()\n",
    "\n",
    "# 初始化紀錄用變數\n",
    "all_rewards = []   # 每回合的總報酬紀錄清單\n",
    "all_steps = []     # 每回合的總步數紀錄清單\n",
    "total_rewards = 0  # 當前回合的累積報酬\n",
    "total_steps = 0    # 當前回合的累積步數\n",
    "no = 50            # 設定要執行的回合數為50\n",
    "\n",
    "# 建立自訂代理人Agent物件\n",
    "agent = Agent()\n",
    "\n",
    "# 開始執行每回合的模擬\n",
    "# 當回合數大於0時繼續模擬\n",
    "while no > 0:\n",
    "    # 根據當前觀測值，由代理人決定行動\n",
    "    action = agent.act(observation)\n",
    "\n",
    "    # 每執行一步，步數加1\n",
    "    total_steps += 1\n",
    "\n",
    "    # 執行環境一步，並取得新觀測值、報酬、終止標記、截斷標記與資訊\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # 判斷是否為回合結束(包含成功結束或失敗結束)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # 將這一步的報酬累加到當前回合的總報酬中\n",
    "    total_rewards += reward\n",
    "\n",
    "    # 如果回合結束(done為True)\n",
    "    if done:\n",
    "        observation, info = env.reset()    # 重置環境，準備下一回合\n",
    "        all_rewards.append(total_rewards)  # 記錄該回合總報酬\n",
    "        total_rewards = 0                  # 重置報酬累計\n",
    "        all_steps.append(total_steps)      # 記錄該回合總步數\n",
    "        total_steps = 0                    # 重置步數累計\n",
    "        no -= 1                            # 減少剩餘回合數\n",
    "\n",
    "# 關閉環境，釋放資源\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1755782964912,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "Vi7i7eGU0cPd",
    "outputId": "b9750440-d390-4754-ee49-c0cd446b351b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "回合\t報酬\t結果\n",
      "0\t42.0\tLoss\n",
      "1\t58.0\tLoss\n",
      "2\t100.0\tLoss\n",
      "3\t151.0\tLoss\n",
      "4\t88.0\tLoss\n",
      "5\t72.0\tLoss\n",
      "6\t71.0\tLoss\n",
      "7\t111.0\tLoss\n",
      "8\t70.0\tLoss\n",
      "9\t73.0\tLoss\n",
      "10\t124.0\tLoss\n",
      "11\t98.0\tLoss\n",
      "12\t98.0\tLoss\n",
      "13\t75.0\tLoss\n",
      "14\t47.0\tLoss\n",
      "15\t83.0\tLoss\n",
      "16\t80.0\tLoss\n",
      "17\t134.0\tLoss\n",
      "18\t78.0\tLoss\n",
      "19\t122.0\tLoss\n",
      "20\t78.0\tLoss\n",
      "21\t94.0\tLoss\n",
      "22\t45.0\tLoss\n",
      "23\t109.0\tLoss\n",
      "24\t104.0\tLoss\n",
      "25\t140.0\tLoss\n",
      "26\t85.0\tLoss\n",
      "27\t94.0\tLoss\n",
      "28\t108.0\tLoss\n",
      "29\t101.0\tLoss\n",
      "30\t154.0\tLoss\n",
      "31\t48.0\tLoss\n",
      "32\t99.0\tLoss\n",
      "33\t96.0\tLoss\n",
      "34\t67.0\tLoss\n",
      "35\t96.0\tLoss\n",
      "36\t90.0\tLoss\n",
      "37\t43.0\tLoss\n",
      "38\t45.0\tLoss\n",
      "39\t74.0\tLoss\n",
      "40\t72.0\tLoss\n",
      "41\t122.0\tLoss\n",
      "42\t76.0\tLoss\n",
      "43\t71.0\tLoss\n",
      "44\t114.0\tLoss\n",
      "45\t75.0\tLoss\n",
      "46\t90.0\tLoss\n",
      "47\t72.0\tLoss\n",
      "48\t76.0\tLoss\n",
      "49\t175.0\tLoss\n"
     ]
    }
   ],
   "source": [
    "# 顯示每回合的執行結果，包括回合數、累積報酬和結果判定\n",
    "print('回合\\t報酬\\t結果')\n",
    "\n",
    "# 判斷結果：如果步數達到或超過200，視為勝利，否則為失敗\n",
    "for i, (rewards, steps) in enumerate(zip(all_rewards, all_steps)):\n",
    "    result = 'Win' if steps >= 200 else 'Loss'\n",
    "    \n",
    "    # 輸出回合編號、累積報酬和結果\n",
    "    print(f'{i}\\t{rewards}\\t{result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1755782964915,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "48qKwfuT0cPe"
   },
   "outputs": [],
   "source": [
    "# 匯入numpy套件，使用數學運算及陣列操作\n",
    "import numpy as np\n",
    "\n",
    "# 定義函式play，接受兩個參數：環境env和策略policy\n",
    "def play(env, policy):\n",
    "    # 重置環境，取得初始觀測值與資訊\n",
    "    observation, info = env.reset()\n",
    "\n",
    "    done = False       # 初始化回合狀態為未結束\n",
    "    score = 0          # 紀錄總得分\n",
    "    observations = []  # 儲存所有觀測值(狀態)\n",
    "\n",
    "    # 最多執行5000步，避免無限迴圈\n",
    "    for _ in range(5000):\n",
    "        # 將每一步的觀測值存入observations(轉成list方便儲存)\n",
    "        observations += [observation.tolist()]\n",
    "\n",
    "        # 如果回合結束(桿子倒下或時間到)，就跳出迴圈\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # 根據策略進行動作選擇(策略是一個向量)\n",
    "        outcome = np.dot(policy, observation)  # 將策略與觀測值作內積，決定行動方向\n",
    "        action = 1 if outcome > 0 else 0       # 若結果>0就往右(1)，否則往左(0)\n",
    "\n",
    "        # 執行動作，觸發下一步，並獲得新的觀測值與狀態資訊\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated  # 若已終止或截斷則表示回合結束\n",
    "        score += reward                 # 將當前步驟的報酬累加到總分\n",
    "\n",
    "    # 回傳總得分與所有觀測紀錄\n",
    "    return score, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1755782965017,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "4focj29K0cPe",
    "outputId": "74a10bab-8bae-48aa-b4b2-fefe8d7889aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score: 261.0\n"
     ]
    }
   ],
   "source": [
    "# 匯入numpy套件，使用數學運算及陣列操作\n",
    "import numpy as np\n",
    "\n",
    "# 初始化max為0，用來記錄目前最高分數、對應的觀察紀錄和策略\n",
    "max = (0, [], [])\n",
    "\n",
    "# 重複執行10次，進行10回合訓練\n",
    "for _ in range(10):\n",
    "    # 產生一組長度為4的隨機策略向量，元素為介於[0, 1)的浮點數\n",
    "    policy = np.random.rand(1, 4)\n",
    "\n",
    "    # 用該策略開始遊戲，取得分數和觀察紀錄\n",
    "    score, observations = play(env, policy)\n",
    "\n",
    "    # 若本回合得分高於目前最高分數，則更新最高分數與相關資料\n",
    "    if score > max[0]:\n",
    "        max = (score, observations, policy)\n",
    "\n",
    "# 輸出最高得分\n",
    "print('Max Score:', max[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1755782965308,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "VeVrRFvh0cPf",
    "outputId": "31f0677f-5651-4b78-f945-3c69b3856af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Score: 500.0\n"
     ]
    }
   ],
   "source": [
    "# 匯入numpy套件，使用數學運算及陣列操作\n",
    "import numpy as np\n",
    "\n",
    "# 初始化max為0，用來記錄目前最高分數、對應的觀察紀錄和策略\n",
    "max = (0, [], [])\n",
    "\n",
    "# 重複執行100次，進行100回合訓練\n",
    "for _ in range(100):\n",
    "    # 產生一組長度為4的隨機策略向量，元素為介於[-0.5, 0.5)的浮點數\n",
    "    policy = np.random.rand(1, 4) - 0.5\n",
    "\n",
    "    # 用該策略開始遊戲，取得分數和觀察紀錄\n",
    "    score, observations = play(env, policy)\n",
    "\n",
    "    # 若本回合得分高於目前最高分數，則更新最高分數與相關資料\n",
    "    if score > max[0]:\n",
    "        max = (score, observations, policy)\n",
    "\n",
    "# 輸出最高得分\n",
    "print('Max Score:', max[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVTSThYS0cPf"
   },
   "source": [
    "## 以最大分數的policy進行實驗，驗證最佳策略是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1755782965336,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "z3dzNyIP0cPf",
    "outputId": "c0fd273b-0f32-4e0c-9609-3bc60da405e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04918937, -0.08512049,  0.33125203,  0.03216584]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得目前最高分數對應的最佳策略(policy)\n",
    "policy = max[2]  # 從max元組中取出最佳策略，索引2為策略\n",
    "policy           # 顯示最佳策略的內容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njRdyawH0cPf"
   },
   "source": [
    "## 以最佳策略取代隨機policy，進行10回合驗證    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1755782965535,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "YNGPNOOG0cPg",
    "outputId": "fa9bcac3-61b7-4339-aaf8-8a056b93ce42",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  500.0\n",
      "Score:  215.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  105.0\n",
      "Score:  147.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n",
      "Score:  500.0\n"
     ]
    }
   ],
   "source": [
    "# 重複執行10次，使用相同的最佳策略測試遊戲表現\n",
    "for _ in range(10):\n",
    "    # 用該策略開始遊戲，取得分數和觀察紀錄\n",
    "    score, observations = play(env, policy)\n",
    "\n",
    "    # 輸出每次遊戲的得分\n",
    "    print('Score: ', score)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
