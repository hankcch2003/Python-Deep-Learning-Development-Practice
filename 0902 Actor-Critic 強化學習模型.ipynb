{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7352,
     "status": "ok",
     "timestamp": 1756812210995,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "4rqZM7akZKU4",
    "outputId": "73e2fee2-e503-461b-cb6c-ee8329919fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym == 0.26.2\n",
      "gym-notices == 0.1.0\n",
      "gymnasium == 1.1.1\n",
      "keras == 3.10.0\n",
      "tensorflow == 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# 匯入warnings模組，用於忽略不必要的警告訊息\n",
    "import warnings\n",
    "\n",
    "# 忽略所有的警告訊息，讓輸出結果更乾淨\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 引入pkg_resources模組，用於取得目前環境中已安裝的套件資訊\n",
    "import pkg_resources\n",
    "\n",
    "# 設定一個要查詢的套件清單，用於比對目前已安裝的套件\n",
    "packages_to_check = ['keras', 'tensorflow', 'gym']\n",
    "\n",
    "# 逐一檢查目前環境中所有已安裝的套件\n",
    "for dist in pkg_resources.working_set:\n",
    "    # 取得套件名稱並轉成小寫，以便進行不區分大小寫的比對\n",
    "    name = dist.project_name.lower()\n",
    "\n",
    "    # 判斷目前的套件名稱是否包含在我們要查詢的清單中\n",
    "    if any(pkg in name for pkg in packages_to_check):\n",
    "        # 如果符合條件，則印出該套件的名稱和版本號\n",
    "        print(f\"{dist.project_name} == {dist.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# 匯入numpy套件，用於數值計算與陣列處理\n",
    "import numpy as np\n",
    "\n",
    "# 解決numpy版本更新後移除np.bool和np.bool8的問題\n",
    "if not hasattr(np, 'bool'):   # 如果np模組中沒有bool屬性，則將其指向內建的bool類型\n",
    "    np.bool = bool\n",
    "if not hasattr(np, 'bool8'):  # 如果np模組中沒有bool8屬性，則將其指向np.bool\n",
    "    np.bool8 = np.bool\n",
    "\n",
    "# 匯入os模組，用於與作業系統互動\n",
    "import os\n",
    "\n",
    "# 指定Keras使用TensorFlow作為後端運算引擎\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# 匯入gym套件，用於建立和操作強化學習環境\n",
    "import gym\n",
    "\n",
    "# 匯入TensorFlow套件，用於深度學習模型的建構與訓練\n",
    "import tensorflow as tf\n",
    "\n",
    "# 從TensorFlow中匯入Keras模組，用於建構和訓練神經網路模型\n",
    "from tensorflow import keras\n",
    "\n",
    "# 從Keras中匯入layers模組，用於建立神經網路的各種層(例如：Dense、Conv等)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# 從Keras中匯入backend模組，提供底層張量操作功能(例如：log、expand_dims等)\n",
    "from tensorflow.keras import backend as ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "executionInfo": {
     "elapsed": 5517,
     "status": "error",
     "timestamp": 1756812153584,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "q5hPUdRiY80P",
    "outputId": "58dda9ef-a05b-42f1-e949-9bcacffd5d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 11.57 at episode 10\n",
      "running reward: 32.05 at episode 20\n",
      "running reward: 46.81 at episode 30\n",
      "running reward: 69.23 at episode 40\n",
      "running reward: 133.63 at episode 50\n",
      "running reward: 157.21 at episode 60\n",
      "Solved at episode 62!\n"
     ]
    }
   ],
   "source": [
    "# 設定強化學習的超參數\n",
    "seed = 42                      # 隨機種子，確保實驗可重現\n",
    "gamma = 0.99                   # 折扣因子，影響未來獎勵的權重\n",
    "max_steps_per_episode = 10000  # 每個回合的最大步數，防止無限迴圈\n",
    "\n",
    "# 建立CartPole-v0環境\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# 重設環境並設定隨機種子\n",
    "env.reset(seed = seed)\n",
    "\n",
    "# 取得float32類型的最小正數，用於避免除以零或數值不穩定的錯誤\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# 定義神經網路的結構與參數\n",
    "num_inputs = 4    # CartPole環境的觀測空間維度\n",
    "num_actions = 2   # CartPole環境的動作空間維度(左或右)\n",
    "num_hidden = 128  # 神經網路隱藏層的單元數\n",
    "\n",
    "# 建立輸入層，輸入形狀為(num_inputs,)，代表神經網路接受的狀態空間維度\n",
    "inputs = layers.Input(shape = (num_inputs,))\n",
    "\n",
    "# 建立一個全連接層，包含num_hidden個神經元，激活函數為ReLU\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "\n",
    "# 建立行動輸出層(Actor)，包含num_actions個神經元，使用softmax激活函數輸出行動機率分布\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "\n",
    "# 建立評估輸出層(Critic)，包含1個神經元，輸出狀態價值估計\n",
    "critic = layers.Dense(1)(common)\n",
    "\n",
    "# 建立整體模型，輸入為inputs，輸出包含action和critic兩個分支\n",
    "model = keras.Model(inputs = inputs, outputs = [action, critic])\n",
    "\n",
    "# 定義Adam優化器，學習率設為0.01\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.01)\n",
    "\n",
    "# 定義Huber損失函數，用於計算Critic的損失\n",
    "huber_loss = keras.losses.Huber()\n",
    "\n",
    "# 用於記錄每個時間步的行動機率、Critic預測值和獎勵\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "\n",
    "# 初始化累積獎勵和回合計數器，方便追蹤訓練進度\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "# 持續訓練直到任務被解決(例如達到指定分數)\n",
    "while True:\n",
    "    # 重置環境並取得初始狀態(reset傳回值為tuple，新版gym需取[0])\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    # 初始化本回合的累積獎勵\n",
    "    episode_reward = 0\n",
    "\n",
    "    # 使用TensorFlow的GradientTape追蹤梯度，方便後續反向傳播\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 在每個回合中持續與環境互動，直到達到最大步數或遊戲結束\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            # 將環境狀態轉成張量，並新增batch維度以符合模型輸入格式\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # 呼叫模型，輸出行動機率(action_probs)與狀態價值估計(critic_value)\n",
    "            action_probs, critic_value = model(state)\n",
    "\n",
    "            # 記錄評估器預測的狀態價值\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # 根據行動機率分布，隨機採取一個動作\n",
    "            action = np.random.choice(num_actions, p = np.squeeze(action_probs.numpy()))\n",
    "\n",
    "            # 記錄所採樣行動的log機率，用於計算策略梯度\n",
    "            action_probs_history.append(ops.log(action_probs[0, action]))\n",
    "\n",
    "            # 根據選定的動作與當前狀態與環境互動，取得新狀態與獎勵\n",
    "            state, reward, done, *_ = env.step(action)\n",
    "\n",
    "            # 紀錄獲得的獎勵\n",
    "            rewards_history.append(reward)\n",
    "\n",
    "            # 累積本回合的獎勵\n",
    "            episode_reward += reward\n",
    "\n",
    "            # 若回合結束，則跳出迴圈\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # 使用指數移動平均更新running_reward，作為學習進度指標\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # 用來存放每個時間步的折扣回報(未來回報的加權和)\n",
    "        returns = []\n",
    "\n",
    "        # 初始化折扣回報的累積和為0\n",
    "        discounted_sum = 0\n",
    "\n",
    "        # 反向遍歷歷史獎勵列表，計算每個時間點的折扣回報\n",
    "        for r in rewards_history[::-1]:\n",
    "            # 折扣回報 = 當前獎勵 + 折扣因子 * 之前的折扣回報\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "\n",
    "            # 將計算結果插入列表開頭，保持時間順序\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # 將折扣回報轉成numpy陣列\n",
    "        returns = np.array(returns)\n",
    "\n",
    "        # 對折扣回報進行標準化，減少數值偏差，提高訓練的穩定性\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "\n",
    "        # 將標準化後的折扣回報轉換為列表格式，方便後續操作\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # 將行動機率歷史、Critic預測值和正規化折扣回報打包，方便後續計算損失\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "\n",
    "        # 初始化演員(Actor)損失和評論者(Critic)損失的列表\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "\n",
    "        # 遍歷每個時間步的資料，計算Actor與Critic的損失\n",
    "        for log_prob, value, ret in history:\n",
    "            # 計算差異：實際折扣回報與Critic預測值的差距\n",
    "            diff = ret - value\n",
    "\n",
    "            # Actor損失：使模型更偏向選擇帶來更高回報的行動，故乘以負對數機率與差異\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "\n",
    "            # Critic損失：使用Huber損失函數衡量預測值與實際回報的誤差，幫助Critic學習更準確的價值估計\n",
    "            critic_losses.append(huber_loss(ops.expand_dims(value, 0), ops.expand_dims(ret, 0)))\n",
    "\n",
    "        # 計算總損失：Actor損失加上Critic損失\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "\n",
    "        # 計算損失對模型參數的梯度\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "        # 使用優化器根據梯度更新模型參數\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # 清空儲存行動機率、Critic值與獎勵的歷史資料，為下一回合做準備\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # 紀錄已完成的回合數\n",
    "    episode_count += 1\n",
    "\n",
    "    # 每執行10回合，輸出目前的平均獎勵(running_reward)與當前回合數，方便觀察訓練進展\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    # 若移動平均獎勵超過195，代表模型已成功完成任務，輸出回合數並結束訓練\n",
    "    if running_reward > 195:\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMJWs2fwrqUP+hbtnSecvmc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
