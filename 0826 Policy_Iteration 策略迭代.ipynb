{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2mvWsHszQY2"
   },
   "source": [
    "# Grid World迷宮之策略循環(Policy Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 從IPython.display模組匯入Image類別，用來在Jupyter Notebook中顯示圖片\n",
    "from IPython.display import Image\n",
    "\n",
    "# 載入並顯示位於當前目錄中的grid_world.png圖片\n",
    "Image('./grid_world.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1756212715894,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "xTUQN_4szUqk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lib/__init__.py\n",
      "lib/__pycache__/\n",
      "lib/__pycache__/__init__.cpython-38.pyc\n",
      "lib/__pycache__/__init__.cpython-39.pyc\n",
      "lib/__pycache__/plotting.cpython-38.pyc\n",
      "lib/__pycache__/plotting.cpython-39.pyc\n",
      "lib/atari/\n",
      "lib/atari/__init__.py\n",
      "lib/atari/helpers.py\n",
      "lib/atari/state_processor.py\n",
      "lib/envs/\n",
      "lib/envs/.ipynb_checkpoints/\n",
      "lib/envs/.ipynb_checkpoints/gridworld-checkpoint.py\n",
      "lib/envs/__init__.py\n",
      "lib/envs/__pycache__/\n",
      "lib/envs/__pycache__/__init__.cpython-36.pyc\n",
      "lib/envs/__pycache__/__init__.cpython-38.pyc\n",
      "lib/envs/__pycache__/__init__.cpython-39.pyc\n",
      "lib/envs/__pycache__/blackjack.cpython-38.pyc\n",
      "lib/envs/__pycache__/blackjack.cpython-39.pyc\n",
      "lib/envs/__pycache__/cliff_walking.cpython-36.pyc\n",
      "lib/envs/__pycache__/cliff_walking.cpython-38.pyc\n",
      "lib/envs/__pycache__/cliff_walking.cpython-39.pyc\n",
      "lib/envs/__pycache__/discrete.cpython-38.pyc\n",
      "lib/envs/__pycache__/discrete.cpython-39.pyc\n",
      "lib/envs/__pycache__/gridworld.cpython-38.pyc\n",
      "lib/envs/__pycache__/gridworld.cpython-39.pyc\n",
      "lib/envs/__pycache__/gridworld0.cpython-39.pyc\n",
      "lib/envs/__pycache__/windy_gridworld.cpython-38.pyc\n",
      "lib/envs/__pycache__/windy_gridworld.cpython-39.pyc\n",
      "lib/envs/blackjack.py\n",
      "lib/envs/cliff_walking.py\n",
      "lib/envs/discrete.py\n",
      "lib/envs/gridworld.py\n",
      "lib/envs/windy_gridworld.py\n",
      "lib/plotting.py\n"
     ]
    }
   ],
   "source": [
    "# 匯入zipfile模組，用來處理.zip壓縮檔案\n",
    "import zipfile\n",
    "\n",
    "# 使用with敘述開啟lib.zip壓縮檔案，確保使用完自動關閉\n",
    "with zipfile.ZipFile(\"lib.zip\") as zf:\n",
    "    # 使用infolist()取得壓縮檔中所有檔案的資訊(ZipInfo物件)\n",
    "    for info in zf.infolist():\n",
    "    \n",
    "        # 因為某些zip壓縮檔的檔名是以cp437編碼(特別是從Windows系統壓縮的)\n",
    "        # 所以這裡先將原本的檔名(字串)用cp437編碼為bytes，再用utf-8解碼為正確的字串\n",
    "        # 這樣可以正確顯示中文或特殊字元的檔名\n",
    "        print(info.filename.encode('cp437').decode('utf-8'))\n",
    "\n",
    "        # 將目前這個檔案(info)解壓縮到target_folder資料夾中\n",
    "        zf.extract(info, \"target_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1756212755033,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "WNCCjsI6zQY5",
    "outputId": "fe4e89a3-19be-4678-a17f-d0bc1fba2912"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# 匯入numpy套件，用於數值計算與陣列處理\n",
    "import numpy as np\n",
    "\n",
    "# 從lib.envs.gridworld模組中匯入自訂的GridworldEnv類別\n",
    "# 這代表有一個自定義的「網格世界」強化學習環境，位於專案的lib/envs/gridworld.py檔案中\n",
    "from lib.envs.gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hankc\\AppData\\Local\\Temp\\ipykernel_11212\\585787934.py:3: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, 'bool'):\n"
     ]
    }
   ],
   "source": [
    "# 修正np.bool已被移除的問題(NumPy1.20+不再支援np.bool)\n",
    "# 如果當前NumPy沒有np.bool屬性(新版會移除)\n",
    "if not hasattr(np, 'bool'):\n",
    "    # 則將np.bool指定為內建的bool，等效於舊版的np.bool行為\n",
    "    np.bool = bool  # 等價替代，讓使用np.bool的舊程式碼能正常執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1756212780078,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "SEbTmDSWzQY7",
    "outputId": "dcc0b838-f315-4815-dd71-47e1bc25b446"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hankc\\anaconda3\\Lib\\site-packages\\gym\\utils\\seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# 建立一個GridWorld強化學習環境實例\n",
    "env = GridworldEnv()\n",
    "\n",
    "# 動作對應的數值編碼(Action Encoding)：\n",
    "# 在強化學習環境中，每個數字代表智能體(Agent)的一個移動方向\n",
    "\n",
    "# UP = 0     # 向上移動，代表智能體往上方的格子前進\n",
    "# RIGHT = 1  # 向右移動，代表智能體往右方的格子前進\n",
    "# DOWN = 2   # 向下移動，代表智能體往下方的格子前進\n",
    "# LEFT = 3   # 向左移動，代表智能體往左方的格子前進"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nB0YNQYozQY8"
   },
   "source": [
    "## 策略評估函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1756213120771,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "f574Yxd3zQY8"
   },
   "outputs": [],
   "source": [
    "# 定義一個策略評估(Policy Evaluation)函式\n",
    "# 用來在給定策略下，計算每個狀態的狀態價值(Value Function V)\n",
    "# 此版本會根據設定的收斂門檻theta自動停止\n",
    "def policy_eval(policy, env, discount_factor = 1.0, theta = 0.00001):\n",
    "\n",
    "    # 初始化狀態值函數V為全0，大小等於環境的狀態數env.nS\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    # 建立一個V的複製版本，用來暫存每次更新後的值\n",
    "    V1 = np.copy(V)\n",
    "\n",
    "    # 持續迭代直到狀態值函數的變化小於設定的門檻theta\n",
    "    while True:\n",
    "        # 每次迭代開始時，將最大差異(delta)初始化為0\n",
    "        delta = 0\n",
    "\n",
    "        # 遍歷每一個狀態s\n",
    "        for s in range(env.nS):\n",
    "            # 用來暫存狀態s的新價值\n",
    "            v = 0\n",
    "\n",
    "            # 遍歷所有動作a及其在該狀態下的選擇機率action_prob\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # 遍歷在狀態s執行動作a的所有可能轉移(由環境模型env.P提供)\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # 套用貝爾曼期望方程式計算狀態價值：\n",
    "                    # v += 該動作機率 × 轉移機率 × [即時獎勵 + 折扣後的下一狀態價值]\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            # 更新最大差異(delta)，用來檢查是否收斂\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "\n",
    "            # 將計算得到的v更新到對應狀態的暫存V1中\n",
    "            V1[s] = v\n",
    "\n",
    "        # 將更新完的V1複製回V，準備下一次迭代\n",
    "        V = np.copy(V1)\n",
    "\n",
    "        # 若最大變動量(delta)小於門檻theta，表示已收斂，停止迴圈\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # 所有迭代完成後，傳回狀態值函數V的最終結果\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqHoe3TKzQY8"
   },
   "source": [
    "## 策略改善函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756213121594,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "QoZyApJTzQY9"
   },
   "outputs": [],
   "source": [
    "# 定義一個策略改進(Policy Improvement)函式\n",
    "# 用來不斷優化給定的初始策略，直到找到最穩定的最優策略\n",
    "# 此函式會搭配策略評估函式(policy_eval_fn)來進行策略疊代\n",
    "def policy_improvement(env, policy_eval_fn = policy_eval, discount_factor = 1.0, max_iterations = 100, theta = 0.1):\n",
    "\n",
    "    # 定義一個一步前瞻(one-step lookahead)函式\n",
    "    # 計算給定狀態s下，每個行動a對應的行動值(Action-Value)\n",
    "    def one_step_lookahead(state, V):\n",
    "        # 初始化每個動作的價值為0\n",
    "        A = np.zeros(env.nA)\n",
    "\n",
    "        # 遍歷所有動作a\n",
    "        for a in range(env.nA):\n",
    "            # 根據環境模型，計算該動作a的期望回報\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                # 行動值Q(s, a) = Σ[P(s'|s, a) * (r + γ * V(s'))]\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "\n",
    "    # 初始化策略為均勻隨機策略：每個狀態均勻選擇所有動作\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "    # 不斷進行策略改進，直到策略收斂為止或超過最大迭代次數\n",
    "    for _ in range(max_iterations):\n",
    "        # 進行策略評估：計算在當前策略下每個狀態的狀態價值函數V\n",
    "        V = policy_eval_fn(policy, env, discount_factor, theta)\n",
    "\n",
    "        # 設定策略穩定標誌為True(若發現更好的策略會設為False)\n",
    "        policy_stable = True\n",
    "\n",
    "        # 遍歷每一個狀態\n",
    "        for s in range(env.nS):\n",
    "            # 記錄當前策略在該狀態下選擇的行動\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "\n",
    "            # 計算該狀態下，每個可能動作的行動值\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "\n",
    "            # 選擇價值最大的動作(即貪婪策略)\n",
    "            best_a = np.argmax(action_values)\n",
    "\n",
    "            # 若新的最佳動作與原策略不同，則策略不穩定\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "\n",
    "            # 更新策略：只保留最佳動作(其他機率為0)\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "\n",
    "        # 若策略穩定(沒有任何狀態的策略發生變動)，則結束迴圈並傳回結果\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    # 傳回最優策略與最終的狀態值函數\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1756213125106,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "tphoOQ5HzQY9",
    "outputId": "7c49607c-e734-4f9f-ac87-f705f81b816f"
   },
   "outputs": [],
   "source": [
    "# 執行策略循環(Policy Iteration)演算法\n",
    "# 呼叫前面定義的policy_improvement函式，傳入環境env\n",
    "# 傳回最終的最優策略policy以及對應的狀態價值函數v\n",
    "policy, v = policy_improvement(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1756213134268,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "_8R-Fo0ezQY-",
    "outputId": "9ce9e729-4862-48bd-825d-cd92e44e1752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略機率分配:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "4x4 策略機率分配(0 = up, 1 = right, 2 = down, 3 = left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "\n",
      "4x4 狀態值函數:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 顯示最終策略的機率分配(每個狀態對每個動作的選擇機率)\n",
    "print(\"策略機率分配:\")\n",
    "print(policy)\n",
    "print(\"\\n\")\n",
    "\n",
    "# 顯示每個狀態下選擇機率最高的動作(即最優策略)\n",
    "# 將一維動作索引轉成4x4矩陣的形式(對應FrozenLake的地圖)\n",
    "# 動作代號：0 = ↑(up), 1 = →(right), 2 = ↓(down), 3 = ←(left)\n",
    "\n",
    "print(\"4x4 策略機率分配(0 = up, 1 = right, 2 = down, 3 = left):\")\n",
    "print(np.reshape(np.argmax(policy, axis = 1), env.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "# 顯示最終狀態值函數(每個格子狀態的預期累積報酬)\n",
    "# 將一維的狀態值向量v轉成4x4矩陣的形式以對應地圖視覺結構\n",
    "print(\"4x4 狀態值函數:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1756213167302,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "qiyxuoA7zQY_",
    "outputId": "2aff61b9-c162-41ab-cfc5-6c10c5536915"
   },
   "outputs": [],
   "source": [
    "# 驗證計算出的狀態值函數v是否與預期結果expected_v接近(小數點後2位)\n",
    "expected_v = np.array([0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bFPmOuAzQZA"
   },
   "source": [
    "## Windy Grid World 迷宮測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1756213192169,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "X17SUeHOzQZA",
    "outputId": "cf542c20-c8ee-439e-c5e8-08e32cf990a2"
   },
   "outputs": [],
   "source": [
    "# 匯入不同的強化學習環境模組(需事先安裝或從OpenAI Gym擴充)\n",
    "from lib.envs.windy_gridworld import WindyGridworldEnv  # 風之格子世界\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv      # 懸崖走路環境\n",
    "from lib.envs.gridworld import GridworldEnv             # 基本格子世界環境\n",
    "\n",
    "# 初始化環境：這裡使用的是CliffWalking(懸崖走路)環境\n",
    "# 起點在左下角，終點在右下角，中間的懸崖會導致掉落並受到負獎勵\n",
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LjO28d75zQZB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, -1.0, False)],\n",
       "  1: [(1.0, 1, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, False)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, False)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 4, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 16, -1.0, False)],\n",
       "  3: [(1.0, 3, -1.0, False)]},\n",
       " 5: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 17, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 18, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 8, -1.0, False)],\n",
       "  2: [(1.0, 19, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 20, -1.0, False)],\n",
       "  3: [(1.0, 7, -1.0, False)]},\n",
       " 9: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 21, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 22, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 11, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 23, -1.0, False)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 0, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 24, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 25, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, False)],\n",
       "  2: [(1.0, 26, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 16, -1.0, False)],\n",
       "  2: [(1.0, 27, -1.0, False)],\n",
       "  3: [(1.0, 14, -1.0, False)]},\n",
       " 16: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 17, -1.0, False)],\n",
       "  2: [(1.0, 28, -1.0, False)],\n",
       "  3: [(1.0, 15, -1.0, False)]},\n",
       " 17: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 18, -1.0, False)],\n",
       "  2: [(1.0, 29, -1.0, False)],\n",
       "  3: [(1.0, 16, -1.0, False)]},\n",
       " 18: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 19, -1.0, False)],\n",
       "  2: [(1.0, 30, -1.0, False)],\n",
       "  3: [(1.0, 17, -1.0, False)]},\n",
       " 19: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 20, -1.0, False)],\n",
       "  2: [(1.0, 31, -1.0, False)],\n",
       "  3: [(1.0, 18, -1.0, False)]},\n",
       " 20: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 21, -1.0, False)],\n",
       "  2: [(1.0, 32, -1.0, False)],\n",
       "  3: [(1.0, 19, -1.0, False)]},\n",
       " 21: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 22, -1.0, False)],\n",
       "  2: [(1.0, 33, -1.0, False)],\n",
       "  3: [(1.0, 20, -1.0, False)]},\n",
       " 22: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 23, -1.0, False)],\n",
       "  2: [(1.0, 34, -1.0, False)],\n",
       "  3: [(1.0, 21, -1.0, False)]},\n",
       " 23: {0: [(1.0, 11, -1.0, False)],\n",
       "  1: [(1.0, 23, -1.0, False)],\n",
       "  2: [(1.0, 35, -1.0, False)],\n",
       "  3: [(1.0, 22, -1.0, False)]},\n",
       " 24: {0: [(1.0, 12, -1.0, False)],\n",
       "  1: [(1.0, 25, -1.0, False)],\n",
       "  2: [(1.0, 36, -1.0, False)],\n",
       "  3: [(1.0, 24, -1.0, False)]},\n",
       " 25: {0: [(1.0, 13, -1.0, False)],\n",
       "  1: [(1.0, 26, -1.0, False)],\n",
       "  2: [(1.0, 37, -100.0, True)],\n",
       "  3: [(1.0, 24, -1.0, False)]},\n",
       " 26: {0: [(1.0, 14, -1.0, False)],\n",
       "  1: [(1.0, 27, -1.0, False)],\n",
       "  2: [(1.0, 38, -100.0, True)],\n",
       "  3: [(1.0, 25, -1.0, False)]},\n",
       " 27: {0: [(1.0, 15, -1.0, False)],\n",
       "  1: [(1.0, 28, -1.0, False)],\n",
       "  2: [(1.0, 39, -100.0, True)],\n",
       "  3: [(1.0, 26, -1.0, False)]},\n",
       " 28: {0: [(1.0, 16, -1.0, False)],\n",
       "  1: [(1.0, 29, -1.0, False)],\n",
       "  2: [(1.0, 40, -100.0, True)],\n",
       "  3: [(1.0, 27, -1.0, False)]},\n",
       " 29: {0: [(1.0, 17, -1.0, False)],\n",
       "  1: [(1.0, 30, -1.0, False)],\n",
       "  2: [(1.0, 41, -100.0, True)],\n",
       "  3: [(1.0, 28, -1.0, False)]},\n",
       " 30: {0: [(1.0, 18, -1.0, False)],\n",
       "  1: [(1.0, 31, -1.0, False)],\n",
       "  2: [(1.0, 42, -100.0, True)],\n",
       "  3: [(1.0, 29, -1.0, False)]},\n",
       " 31: {0: [(1.0, 19, -1.0, False)],\n",
       "  1: [(1.0, 32, -1.0, False)],\n",
       "  2: [(1.0, 43, -100.0, True)],\n",
       "  3: [(1.0, 30, -1.0, False)]},\n",
       " 32: {0: [(1.0, 20, -1.0, False)],\n",
       "  1: [(1.0, 33, -1.0, False)],\n",
       "  2: [(1.0, 44, -100.0, True)],\n",
       "  3: [(1.0, 31, -1.0, False)]},\n",
       " 33: {0: [(1.0, 21, -1.0, False)],\n",
       "  1: [(1.0, 34, -1.0, False)],\n",
       "  2: [(1.0, 45, -100.0, True)],\n",
       "  3: [(1.0, 32, -1.0, False)]},\n",
       " 34: {0: [(1.0, 22, -1.0, False)],\n",
       "  1: [(1.0, 35, -1.0, False)],\n",
       "  2: [(1.0, 46, -100.0, True)],\n",
       "  3: [(1.0, 33, -1.0, False)]},\n",
       " 35: {0: [(1.0, 23, -1.0, False)],\n",
       "  1: [(1.0, 35, -1.0, False)],\n",
       "  2: [(1.0, 47, -1.0, True)],\n",
       "  3: [(1.0, 34, -1.0, False)]},\n",
       " 36: {0: [(1.0, 24, -1.0, False)],\n",
       "  1: [(1.0, 37, -100.0, True)],\n",
       "  2: [(1.0, 36, -1.0, False)],\n",
       "  3: [(1.0, 36, -1.0, False)]},\n",
       " 37: {0: [(1.0, 25, -1.0, False)],\n",
       "  1: [(1.0, 38, -100.0, True)],\n",
       "  2: [(1.0, 37, -100.0, True)],\n",
       "  3: [(1.0, 36, -1.0, False)]},\n",
       " 38: {0: [(1.0, 26, -1.0, False)],\n",
       "  1: [(1.0, 39, -100.0, True)],\n",
       "  2: [(1.0, 38, -100.0, True)],\n",
       "  3: [(1.0, 37, -100.0, True)]},\n",
       " 39: {0: [(1.0, 27, -1.0, False)],\n",
       "  1: [(1.0, 40, -100.0, True)],\n",
       "  2: [(1.0, 39, -100.0, True)],\n",
       "  3: [(1.0, 38, -100.0, True)]},\n",
       " 40: {0: [(1.0, 28, -1.0, False)],\n",
       "  1: [(1.0, 41, -100.0, True)],\n",
       "  2: [(1.0, 40, -100.0, True)],\n",
       "  3: [(1.0, 39, -100.0, True)]},\n",
       " 41: {0: [(1.0, 29, -1.0, False)],\n",
       "  1: [(1.0, 42, -100.0, True)],\n",
       "  2: [(1.0, 41, -100.0, True)],\n",
       "  3: [(1.0, 40, -100.0, True)]},\n",
       " 42: {0: [(1.0, 30, -1.0, False)],\n",
       "  1: [(1.0, 43, -100.0, True)],\n",
       "  2: [(1.0, 42, -100.0, True)],\n",
       "  3: [(1.0, 41, -100.0, True)]},\n",
       " 43: {0: [(1.0, 31, -1.0, False)],\n",
       "  1: [(1.0, 44, -100.0, True)],\n",
       "  2: [(1.0, 43, -100.0, True)],\n",
       "  3: [(1.0, 42, -100.0, True)]},\n",
       " 44: {0: [(1.0, 32, -1.0, False)],\n",
       "  1: [(1.0, 45, -100.0, True)],\n",
       "  2: [(1.0, 44, -100.0, True)],\n",
       "  3: [(1.0, 43, -100.0, True)]},\n",
       " 45: {0: [(1.0, 33, -1.0, False)],\n",
       "  1: [(1.0, 46, -100.0, True)],\n",
       "  2: [(1.0, 45, -100.0, True)],\n",
       "  3: [(1.0, 44, -100.0, True)]},\n",
       " 46: {0: [(1.0, 34, -1.0, False)],\n",
       "  1: [(1.0, 47, -1.0, True)],\n",
       "  2: [(1.0, 46, -100.0, True)],\n",
       "  3: [(1.0, 45, -100.0, True)]},\n",
       " 47: {0: [(1.0, 35, -1.0, False)],\n",
       "  1: [(1.0, 47, -1.0, True)],\n",
       "  2: [(1.0, 47, -1.0, True)],\n",
       "  3: [(1.0, 46, -100.0, True)]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看GridWorld環境的轉移模型P\n",
    "# P是一個嵌套字典，描述在每個狀態執行某個動作後，可能轉移到哪些狀態、機率是多少、是否結束等資訊\n",
    "# 用於實作動態規劃演算法，如：策略評估、策略改進、值迭代、策略迭代等\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1756213198664,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "d7nkr1gPzQZC",
    "outputId": "fc07765a-7aca-4bf3-cdb0-cc3364ec4a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "狀態值函數:\n",
      "[[ -722.51615437  -744.75221796  -776.81661995  -807.75063697\n",
      "   -830.96046916  -843.19413687  -843.19413687  -830.96046916]\n",
      " [ -807.75063697  -776.81661995  -744.75221796  -722.51615437\n",
      "   -781.4158988   -816.39931357  -860.01078979  -898.21777004]\n",
      " [ -925.26784418  -939.09855251  -939.09855251  -925.26784418\n",
      "   -898.21777004  -860.01078979  -816.39931357  -781.4158988 ]\n",
      " [ -886.52414133  -960.9501504  -1030.72960917 -1082.6416538\n",
      "  -1116.18269185 -1132.56158456 -1132.56158456 -1116.18269185]\n",
      " [-1082.6416538  -1030.72960917  -960.9501504   -886.52414133\n",
      "   -998.47773191 -1092.75878447 -1202.5160867  -1269.31647918]\n",
      " [-1308.72852841 -1327.21215404 -1327.21215404 -1308.72852841\n",
      "  -1269.31647918 -1202.5160867  -1092.75878447  -998.47773191]]\n"
     ]
    }
   ],
   "source": [
    "# 隨機策略，機率均等：在每個狀態下，對每個動作(action)賦予相同的機率\n",
    "# np.ones([env.nS, env.nA])創建一個大小為[狀態數量, 動作數量]的矩陣，並將所有值設為1\n",
    "# 然後除以動作數量(env.nA)，使得每個動作的機率均等\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "# 評估隨機策略：計算該隨機策略下每個狀態的狀態價值函數(V)\n",
    "# theta = 22是收斂條件，設定最大變動量的容忍誤差(這裡設定為22較大，可以根據需要調整)\n",
    "v = policy_eval(random_policy, env, theta = 22)\n",
    "\n",
    "# 輸出狀態值函數：顯示該隨機策略下的狀態價值函數(V)\n",
    "# 假設環境的狀態空間是6x8的網格，使用v.reshape()方法將一維向量轉為6x8的格狀顯示方式\n",
    "print(\"狀態值函數:\")\n",
    "print(v.reshape((6, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "error",
     "timestamp": 1756213228087,
     "user": {
      "displayName": "Lewis Yang",
      "userId": "11249979140389061982"
     },
     "user_tz": -480
    },
    "id": "3XMdnBr5zQZC",
    "outputId": "81ff83a3-5b69-4e2d-ab28-2a3e6fae1ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略機率分配:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "狀態值函數:\n",
      "[[ -722.51615437  -744.75221796  -776.81661995  -807.75063697\n",
      "   -830.96046916  -843.19413687  -843.19413687  -830.96046916]\n",
      " [ -807.75063697  -776.81661995  -744.75221796  -722.51615437\n",
      "   -781.4158988   -816.39931357  -860.01078979  -898.21777004]\n",
      " [ -925.26784418  -939.09855251  -939.09855251  -925.26784418\n",
      "   -898.21777004  -860.01078979  -816.39931357  -781.4158988 ]\n",
      " [ -886.52414133  -960.9501504  -1030.72960917 -1082.6416538\n",
      "  -1116.18269185 -1132.56158456 -1132.56158456 -1116.18269185]\n",
      " [-1082.6416538  -1030.72960917  -960.9501504   -886.52414133\n",
      "   -998.47773191 -1092.75878447 -1202.5160867  -1269.31647918]\n",
      " [-1308.72852841 -1327.21215404 -1327.21215404 -1308.72852841\n",
      "  -1269.31647918 -1202.5160867  -1092.75878447  -998.47773191]]\n"
     ]
    }
   ],
   "source": [
    "# 顯示最終策略的機率分配(每個狀態對每個動作的選擇機率)\n",
    "print(\"策略機率分配:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "# 輸出狀態值函數：顯示該隨機策略下的狀態價值函數(V)\n",
    "# 假設環境的狀態空間是6x8的網格，使用v.reshape()方法將一維向量轉為6x8的格狀顯示方式\n",
    "print(\"狀態值函數:\")\n",
    "print(v.reshape((6, 8)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
